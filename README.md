# ğŸš€ open-responses-server

A plug-and-play server that speaks OpenAIâ€™s Responses APIâ€”no matter which AI backend youâ€™re running.  

Ollama? vLLM? LiteLLM? Even OpenAI itself?  
This server bridges them all to the OpenAI ChatCompletions & Responses API interface.  

In plain words:  
ğŸ‘‰ Want to run OpenAIâ€™s Coding Assistant (Codex) or other OpenAI API clients against your own models?  
ğŸ‘‰ Want to experiment with self-hosted LLMs but keep OpenAIâ€™s API compatibility?  

This project makes it happen.  
It handles stateful chat, tool calls, and future features like file search & code interpreterâ€”all behind a familiar OpenAI API.

â¸»

# âœ¨ Why use this?

âœ… Acts as a drop-in replacement for OpenAIâ€™s Responses API.  
âœ… Lets you run any backend AI (Ollama, vLLM, Groq, etc.) with OpenAI-compatible clients.  
âœ… MCP support around both Chat Completions and Responses APIs
âœ… Supports OpenAIâ€™s new Coding Assistant / Codex that requires Responses API.  
âœ… Built for innovators, researchers, OSS enthusiasts.  
âœ… Enterprise-ready: scalable, reliable, and secure for production workloads.

â¸»

ğŸ”¥ Whatâ€™s in & whatâ€™s next?

âœ… Done	ğŸ“ Coming soon
- âœ… Tool call support	.env file support
- âœ… Manual & pipeline tests
- âœ… Docker image build
- âœ… PyPI release	
- ğŸ“ Persistent state (not just in-memory)
- âœ… CLI validation	
- ğŸ“ hosted tools:
  - âœ… MCPs support
  - ğŸ“ Web search: crawl4ai
  - ğŸ“ File upload + search: graphiti
  - ğŸ“ Code interpreter
  - ğŸ“ Computer use APIs

â¸»

# ğŸ—ï¸ Quick Install

Latest release on PyPI:

```
pip install open-responses-server
```

Or install from source:

```
uv venv
uv pip install .
uv pip install -e ".[dev]"  # dev dependencies
```

Run the server:

```
# Using CLI tool (after installation)
otc start

# Or directly from source
uv run src/open_responses_server/cli.py start
```

Docker deployment:

```
# Run with Docker
docker run -p 8080:8080 \
  -e OPENAI_BASE_URL_INTERNAL=http://your-llm-api:8000 \
  -e OPENAI_BASE_URL=http://localhost:8080 \
  -e OPENAI_API_KEY=your-api-key \
  ghcr.io/teabranch/open-responses-server:latest
```

Works great with docker-compose.yaml for Codex + your own model.

â¸»

# ğŸ› ï¸ Configure

Minimal config to connect your AI backend:

```
OPENAI_BASE_URL_INTERNAL=http://localhost:11434  # Ollama, vLLM, Groq, etc.
OPENAI_BASE_URL=http://localhost:8080            # This server's endpoint
OPENAI_API_KEY=sk-mockapikey123456789            # Mock key tunneled to backend
```

Server binding:
```
API_ADAPTER_HOST=0.0.0.0
API_ADAPTER_PORT=8080
```
Optional logging:
```
LOG_LEVEL=INFO
LOG_FILE_PATH=./log/api_adapter.log
```

Configure with CLI tool:
```
# Interactive configuration setup
otc configure
```

Verify setup:
```
# Check if the server is working
curl http://localhost:8080/v1/models
```

â¸»

# ğŸ’¬ Weâ€™d love your support!

If you think this is cool:  
â­ Star the repo.  
ğŸ› Open an issue if somethingâ€™s broken.  
ğŸ¤ Suggest a feature or submit a pull request!  

This is early-stage but already usable in real-world demos.  
Letâ€™s build something powerfulâ€”together.


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=TeaBranch/open-responses-server&type=Date)](https://www.star-history.com/#TeaBranch/open-responses-server&Date)

# Projects using this middleware
- [Agentic Developer MCP Server](https://github.com/teabranch/agentic-developer-mcp) - a wrapper around Codex, transforming Codex into an agentic developer node over a folder. Together with this (ORS) repo, it becomes a link in a tree/chain of developers. 
- [Nvidia jetson devices](https://github.com/OriNachum/autonomous-intelligence/tree/main/local-codex) - docker compose with ollama

â¸»

# ğŸ“š Citations & inspirations

## Referenced projects
- [SearXNG MCP](https://github.com/ihor-sokoliuk/mcp-searxng)
- UncleCode. (2024). Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper [Computer software]. GitHub. [Crawl4AI repo](https://github.com/unclecode/crawl4ai)

## Cite this project

### Code citation
```
@software{open-responses-server,
  author = {TeaBranch},
  title = {open-responses-server: Open-source server bridging any AI provider to OpenAIâ€™s Responses API},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/teabranch/open-responses-server}},
  commit = {use the commit hash youâ€™re working with}
}
```

### Text citation

TeaBranch. (2025). open-responses-server: Open-source server the serves any AI provider with OpenAI ChatCompletions as OpenAI's Responses API and hosted tools. [Computer software]. GitHub. https://github.com/teabranch/open-responses-server

# Links:
- [Python library](https://pypi.org/project/open-responses-server)
- [GitHub repository](https://github.com/teabranch/open-responses-server)
- [GitHub Pages](https://teabranch.github.io/open-responses-server)

# Naming history
This repo had changed names:
- openai-responses-server (Changed to avoid brand name OpenAI)
- open-responses-server 

